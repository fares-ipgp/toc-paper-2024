{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TatX5vySmTU",
        "outputId": "d25e14be-aec5-4c48-f057-02b55b9d3788"
      },
      "outputs": [],
      "source": [
        "#@title Install requirements\n",
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibLIEVxd4aM6"
      },
      "outputs": [],
      "source": [
        "#@title Imports packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from adjustText import adjust_text\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wM6djEBe3R7x"
      },
      "outputs": [],
      "source": [
        "#@title Utils\n",
        "\n",
        "def get_columns_with_prefix(dataframe, prefix):\n",
        "    # Get all column names from the DataFrame\n",
        "    columns = dataframe.columns.tolist()\n",
        "\n",
        "    # Filter columns with the specified prefix\n",
        "    filtered_columns = [column for column in columns if column.startswith(prefix)]\n",
        "\n",
        "    return filtered_columns\n",
        "\n",
        "def compute_reduced_dimension(df, cols, method='t-sne', random_state=42):\n",
        "\n",
        "  df_red = df.copy()\n",
        "\n",
        "  X = df_red[cols]\n",
        "\n",
        "  _x = method + \"_1\"\n",
        "  _y = method + \"_2\"\n",
        "\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "  # method\n",
        "  if method == \"t-sne\":\n",
        "    from sklearn.manifold import TSNE\n",
        "    red = TSNE(n_components=2,\n",
        "               perplexity=30,\n",
        "               n_iter=100000,\n",
        "               learning_rate=0.1,\n",
        "               early_exaggeration=2,\n",
        "               n_iter_without_progress=200,\n",
        "               metric='minkowski',\n",
        "               random_state=random_state)\n",
        "\n",
        "  if method == \"umap\":\n",
        "    import umap\n",
        "    red = red = umap.UMAP(random_state=random_state, n_jobs = 1)\n",
        "\n",
        "  if method == \"pca\":\n",
        "    from sklearn.decomposition import PCA\n",
        "    red = red = PCA(n_components=2)\n",
        "\n",
        "  Xs = StandardScaler().fit_transform(X)\n",
        "\n",
        "  results = red.fit_transform(Xs)\n",
        "  df_red[_x]=pd.Series(results[:,0])\n",
        "  df_red[_y]=pd.Series(results[:,1])\n",
        "\n",
        "  return df_red\n",
        "\n",
        "def plot_2d(ax, df, _x, _y, _s, group_by, label, show_labels = 'false'):\n",
        "\n",
        "  # select rows\n",
        "  df_plot = df.copy()\n",
        "\n",
        "  ax.tick_params(labelrotation=90)\n",
        "\n",
        "  import matplotlib.cm as cm\n",
        "  import itertools\n",
        "\n",
        "  # Define the categories\n",
        "  categories = df_plot[group_by].unique()\n",
        "  palette = sns.color_palette(\"Set1\")\n",
        "  palette_iter = itertools.cycle(palette)\n",
        "  category_colors = {category: next(palette_iter) for category in categories}\n",
        "\n",
        "  sns.scatterplot(data=df_plot,\n",
        "                  x=_x,\n",
        "                  y=_y,\n",
        "                  marker=\".\" ,\n",
        "                  size=1,\n",
        "                  hue=group_by,\n",
        "                  ax=ax,\n",
        "                  palette= category_colors,\n",
        "                  alpha = 0.5,\n",
        "                  legend = False)\n",
        "\n",
        "  sns.scatterplot(data=df_plot,\n",
        "                  x=_x,\n",
        "                  y=_y,\n",
        "                  marker=\"o\" ,\n",
        "                  hue=group_by,\n",
        "                  sizes=(25, 150),\n",
        "                  size=_s,\n",
        "                  ax=ax,\n",
        "                  palette= category_colors,\n",
        "                  alpha = 1,\n",
        "                  legend = True)\n",
        "\n",
        "  # handle categories with very low variance\n",
        "  variances = df_plot.groupby(group_by)[_x].var()\n",
        "  non_zero_var_categories = variances[variances > 0.001].index\n",
        "  df_filtered = df_plot[df_plot[group_by].isin(non_zero_var_categories)]\n",
        "\n",
        "  sns.kdeplot(data=df_filtered,\n",
        "              x=_x,\n",
        "              y=_y,\n",
        "              hue=group_by,\n",
        "              alpha=0.2,\n",
        "              fill=False,\n",
        "              warn_singular=False,\n",
        "              palette= category_colors,\n",
        "              legend = False,\n",
        "              ax=ax)\n",
        "\n",
        "  from adjustText import adjust_text as at\n",
        "  if show_labels == \"true\":\n",
        "    texts = []\n",
        "    colors= df_plot[group_by].map(category_colors)\n",
        "    labels = df_plot[label].astype(str)\n",
        "    for i, txt in enumerate(labels):\n",
        "        texts.append(ax.text(df_plot.at[i,_x], df_plot.at[i,_y], txt, ha='center',alpha=1, fontsize=7, c=colors[i]))\n",
        "\n",
        "    # Adjust text positions to avoid overlap\n",
        "    at(texts, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pI_E7SVK4i6N",
        "outputId": "c0e30e1d-2f2d-4318-beba-575989d40ded"
      },
      "outputs": [],
      "source": [
        "#@title Datos\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/fares-ipgp/toc-paper-2024/main/tocm.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIeixWtos6KF",
        "outputId": "6c1c2001-e8b2-4b06-c8b7-280351049bdf"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "d3sIyX6W6x2U",
        "outputId": "6e3a46c5-c3b0-4a55-ca78-59effda658c7"
      },
      "outputs": [],
      "source": [
        "#@title Features\n",
        "#@markdown Numerical values. Impute NaN as mean.\n",
        "# bioturb\tamb1\tamb2\n",
        "feature_names = []\n",
        "\n",
        "columns_geological = get_columns_with_prefix(df,'geo__')\n",
        "columns_xrd = get_columns_with_prefix(df,'xrd__')\n",
        "columns_label = ['sample']\n",
        "\n",
        "# Numeric feature names\n",
        "feature_names.extend(columns_xrd)\n",
        "\n",
        "# Mean imputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "df[columns_xrd] = imp.fit_transform(df[columns_xrd])\n",
        "df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Targets { run: \"auto\" }\n",
        "\n",
        "# Threshol toc value\n",
        "df.loc[df['y__toc'].notna(),'y__hi_toc'] = df.loc[df['y__toc'].notna(),'y__toc'] > 1\n",
        "df[['sample','y__hi_toc']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U-SPN6SMyV5B",
        "outputId": "5ae69238-83d3-4e74-8c6a-02376ab1a0cc"
      },
      "outputs": [],
      "source": [
        "#@title Importancia de features { run: \"auto\" }\n",
        "\n",
        "#@markdown En la gráfica de la se representan los features ordenados de mayor a menor importancia.\n",
        "#@markdown Las barras representan en índice de importancia. Este índice (F score ANOVA y GINI impurity para Random Forest ).\n",
        "#@markdown La línea azul representa el índice de silhouette promedio resultante de haber seleccionado los N features más significativos.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "#features = \"all\" #@param [\"all\",\"geological\",\"analysis_xrd\"]\n",
        "target = \"y__hi_toc\" #@param [\"y__hi_toc\",\"y__hi_if\",\"y__hi_toc&if\"]\n",
        "threshold = 0.075 #@param {type:\"slider\", min:0, max:1, step:0.025}\n",
        "max_features = 4 #@param {type:\"slider\", min:1, max:15, step:1}\n",
        "\n",
        "method = \"ANOVA\"\n",
        "\n",
        "best_features = {}\n",
        "\n",
        "# Setup figure\n",
        "fig, ax = plt.subplots(1,1)\n",
        "fig.set_size_inches(10,5)\n",
        "\n",
        "  # only use labeled data\n",
        "df_show = df.copy()\n",
        "df_show = df_show.loc[df_show[target].notna()]\n",
        "df_show = df_show.reset_index(drop=True)\n",
        "\n",
        "cols = columns_geological + columns_xrd\n",
        "\n",
        "# features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss= StandardScaler()\n",
        "ss.fit(df_show[cols])\n",
        "\n",
        "X_raw = df_show[cols]\n",
        "X_scaled=pd.DataFrame(ss.transform(X_raw),columns = X_raw.columns)\n",
        "\n",
        "# method\n",
        "cols = cols\n",
        "Xr= X_scaled\n",
        "\n",
        "# target\n",
        "y = df_show[target].astype(bool)\n",
        "\n",
        "if method == \"Random Forest\":\n",
        "    model = RandomForestClassifier(n_estimators=10000, min_samples_split=4, min_samples_leaf=2, criterion='gini')\n",
        "    model.fit(Xr, y)\n",
        "    importance = model.feature_importances_\n",
        "    df_importance = pd.DataFrame({'feature': cols, 'importance': importance})\n",
        "elif method == \"ANOVA\":\n",
        "    importance, p_value = f_classif(Xr, y)\n",
        "    df_importance = pd.DataFrame({'feature': cols, 'importance': importance})\n",
        "elif method == \"LinearSVC\":\n",
        "    model = LinearSVC()\n",
        "    model.fit(Xr, y)\n",
        "    importance = np.abs(model.coef_[0])\n",
        "    df_importance = pd.DataFrame({'feature': cols, 'importance': importance})\n",
        "else:\n",
        "    importance = []\n",
        "\n",
        "df_sorted = df_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "scores=[]\n",
        "num_features=[]\n",
        "\n",
        "for idx, feature in enumerate(df_sorted['feature']):\n",
        "    _best_features= list(df_sorted['feature'].head(idx+1))\n",
        "    df_sil=df_show[ _best_features + [target]].copy()\n",
        "    df_sil['silhouette']=silhouette_samples(df_sil[_best_features],df_sil[target])\n",
        "    scores+=[df_sil.agg({'silhouette': ['mean']})['silhouette']]\n",
        "\n",
        "scores=list(np.array(scores).reshape(-1))\n",
        "#max_features=scores[3:].index(max(scores[3:]))+4\n",
        "\n",
        "ax.tick_params(labelrotation=90)\n",
        "sns.barplot(df_sorted.head(50),x='feature',y='importance',ax=ax)\n",
        "\n",
        "ax2=ax.twinx()\n",
        "ax2.plot(df_sorted.head(50)['feature'],scores[:50])\n",
        "\n",
        "ax.axvline(x=max_features-0.5, color='black', linestyle='--')\n",
        "\n",
        "best_features[method]= list(df_sorted['feature'].head(max_features))\n",
        "print(max_features,silhouette_score(Xr[best_features[method]],y), best_features[method])\n",
        "\n",
        "ax2.set_ylabel('silhouette')\n",
        "ax.set_title('Feature Ranking by ' + method + '\\ntarget: ' + target)\n",
        "fig.tight_layout(pad=3.0)\n",
        "\n",
        "fig.savefig('diagrams/feature_ranking.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fmhcy4xzNRI2"
      },
      "outputs": [],
      "source": [
        "#@title Reducción dimensional - comparación de métodos { run: \"auto\" }\n",
        "\n",
        "features = \"xrd+geo\" #@param [\"xrd+geo\",\"xrd\",\"best_anova\",\"best_rf\"]\n",
        "target = \"y__toc\" #@param [\"y__toc\",\"y__if\",\"y__toc*if\", \"y__hi_toc\", \"y__hi_if\", \"y__hi_toc*if\"]\n",
        "methods=('pca','t-sne','umap')\n",
        "\n",
        "feature_sets = {'xrd+geo': columns_geological + columns_xrd ,\n",
        "                'best_anova': best_features['ANOVA']}\n",
        "\n",
        "fig, axes = plt.subplots(1,len(methods))\n",
        "fig.set_size_inches(21,7)\n",
        "fig.suptitle('Dimensional reduction (' + features + ')', fontsize=20)\n",
        "\n",
        "# select features\n",
        "cols = feature_sets[features]\n",
        "\n",
        "df=compute_reduced_dimension(df, cols, method='t-sne')\n",
        "df=compute_reduced_dimension(df, cols, method='umap')\n",
        "df=compute_reduced_dimension(df, cols, method='pca')\n",
        "\n",
        "# Sweep over q values and plot resuts\n",
        "for idx, method in enumerate(methods):\n",
        "  axes[idx].set_title( 'method: ' + method)\n",
        "  plot_2d(axes[idx], df, method + '_1'  , method + '_2' , target ,'section', 'y__toc', show_labels = 'false')\n",
        "  \n",
        "fig.savefig('diagrams/dimensional_reduction.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        },
        "id": "BPUaoZpDV67Y",
        "outputId": "e9c2d904-7698-4f93-c8da-e9efe43849c0"
      },
      "outputs": [],
      "source": [
        "#@title Selección de features - comparacion { run: \"auto\" }\n",
        "\n",
        "method = \"t-sne\" #@param [\"pca\",\"t-sne\",\"umap\"]\n",
        "target = \"y__toc\" #@param [\"y__toc\",\"y__if\",\"y__toc*if\", \"y__hi_toc\", \"y__hi_if\", \"y__hi_toc*if\"]\n",
        "\n",
        "feature_sets = {#'xrd':columns_xrd,\n",
        "                'xrd+geo': columns_geological + columns_xrd ,\n",
        "                'best_anova': best_features['ANOVA']\n",
        "                #,'best_rf': best_features['Random Forest']\n",
        "                }\n",
        "\n",
        "fig, axes = plt.subplots(1,len(feature_sets))\n",
        "fig.set_size_inches(14,7)\n",
        "fig.suptitle('Feature Selection', fontsize=20)\n",
        "\n",
        "for fidx, features in enumerate(feature_sets):\n",
        "\n",
        "  # select features\n",
        "  cols = feature_sets[features]\n",
        "  df=compute_reduced_dimension(df, cols, method=method)\n",
        "\n",
        "  # Sweep over q values and plot resuts\n",
        "  axes[fidx].set_title(features)\n",
        "  plot_2d(axes[fidx], df, method + '_1'  , method + '_2' , target ,'section', 'sample', show_labels = 'false')\n",
        "  \n",
        "  fig.savefig('diagrams/feature_selection.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "zbXrhfA5FdDV",
        "outputId": "f02a0067-5f53-4b52-c30d-8aa1d4dda5f5"
      },
      "outputs": [],
      "source": [
        "#@title Visualización en dimensión reducida - detalle { run: \"auto\" }\n",
        "\n",
        "features = \"xrd+geo\" #@param [\"xrd+geo\",\"xrd\",\"best_anova\",\"best_rf\"]\n",
        "\n",
        "method = \"t-sne\" #@param [\"pca\",\"t-sne\",\"umap\"]\n",
        "target = \"y__toc\" #@param [\"y__toc\",\"y__if\",\"y__toc*if\", \"y__hi_toc\", \"y__hi_if\", \"y__hi_toc&if\"]\n",
        "\n",
        "feature_sets = {'xrd+geo': columns_geological + columns_xrd,\n",
        "                'best_anova': best_features['ANOVA']}\n",
        "\n",
        "fig, axes = plt.subplots()\n",
        "fig.set_size_inches(10,10)\n",
        "fig.suptitle('Detail view', fontsize=20)\n",
        "\n",
        "# select features\n",
        "cols = feature_sets[features]\n",
        "df=compute_reduced_dimension(df, cols, method=method)\n",
        "\n",
        "# Sweep over q values and plot resuts\n",
        "axes.set_title(features)\n",
        "plot_2d(axes, df, method + '_1'  , method + '_2' , target ,'section', 'sample', show_labels = 'true')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xXuW979XgX8I"
      },
      "outputs": [],
      "source": [
        "#@title Feature sets and target { run: \"auto\" }\n",
        "\n",
        "train_features = \"xrd+geo\" #@param [\"xrd+geo\",\"best_anova\"]\n",
        "target = \"y__hi_toc\" #@param [\"y__hi_toc\",\"y__hi_if\",\"y__hi_toc&if\"]\n",
        "\n",
        "feature_sets = {'xrd+geo': columns_geological + columns_xrd ,\n",
        "                'best_anova': best_features['ANOVA']}\n",
        "\n",
        "# only use labeled data\n",
        "df_train = df.copy()\n",
        "df_train = df_show.loc[df_show[target].notna()]\n",
        "df_train = df_show.reset_index(drop=True)\n",
        "\n",
        "cols =   feature_sets[train_features]\n",
        "X = df_train[cols]\n",
        "\n",
        "# target\n",
        "y = df_train[target].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M9bmu4k2dxAZ"
      },
      "outputs": [],
      "source": [
        "#@title Models { run: \"auto\" }\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature selection\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# Model\n",
        "from sklearn import linear_model\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "pipelines={}\n",
        "param_spaces={}\n",
        "opts={}\n",
        "cv_params={}\n",
        "\n",
        "#\n",
        "# logistic\n",
        "#\n",
        "\n",
        "pipelines['logistic'] = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # StandardScaler preprocessing step\n",
        "    ('classifier', linear_model.LogisticRegression(solver='liblinear'))           # Classification model (SVM in this case)\n",
        "])\n",
        "\n",
        "param_spaces['logistic'] = {\n",
        "    'classifier__C': Real(1e-4, 1e+3, prior='log-uniform'),\n",
        "    'classifier__penalty': Categorical(['l2', 'l1'])\n",
        "}\n",
        "\n",
        "cv_params['logistic']={'n_iter':100,\n",
        "                       'n_points':10}\n",
        "\n",
        "#\n",
        "# SVC\n",
        "#\n",
        "\n",
        "pipelines['svc'] = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # StandardScaler preprocessing step\n",
        "    ('classifier', svm.SVC(probability=True))           # Classification model (SVM in this case)\n",
        "])\n",
        "\n",
        "param_spaces['svc'] = {\n",
        "    'classifier__C': Real(1e-4 , 1e+3, prior='log-uniform'),\n",
        "    'classifier__kernel': Categorical(['sigmoid'])\n",
        "}\n",
        "\n",
        "cv_params['svc']={'n_iter':100,\n",
        "                       'n_points':10}\n",
        "\n",
        "#\n",
        "# KNN\n",
        "#\n",
        "\n",
        "pipelines['knn'] = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # StandardScaler preprocessing step\n",
        "    ('classifier', neighbors.KNeighborsClassifier())           # Classification model\n",
        "])\n",
        "\n",
        "param_spaces['knn'] = {\n",
        "    'classifier__n_neighbors': Integer(2,8),\n",
        "    'classifier__weights': Categorical(['uniform', 'distance']),\n",
        "    'classifier__p': Integer(1,4)\n",
        "}\n",
        "\n",
        "cv_params['knn']={'n_iter':100,\n",
        "                  'n_points':10}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJHoeN3TR9q8",
        "outputId": "fa2d7018-fd85-4b83-cfe0-554b85bd405b"
      },
      "outputs": [],
      "source": [
        "#@title Train models { run: \"auto\" }\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "best_results={}\n",
        "results={}\n",
        "models={}\n",
        "loo = {}\n",
        "\n",
        "def custom_callback(res):\n",
        "    print(f\"Epoch {len(res.func_vals)} - Best score: {res.fun:.6f}, Best params: {res.x}, last params:{res.x_iters[-1]}, last score:{res.func_vals[-1]} \")\n",
        "    #print(res)\n",
        "\n",
        "for pipeline in pipelines:\n",
        "\n",
        "  print(f\"Training {pipeline}\")\n",
        "\n",
        "  # Define Leave-One-Out cross-validation strategy\n",
        "  loo[pipeline] = LeaveOneOut()\n",
        "\n",
        "  opts[pipeline] = BayesSearchCV( pipelines[pipeline],\n",
        "                                  param_spaces[pipeline],\n",
        "                                  cv= loo[pipeline],  # LOO cross-validation\n",
        "                                  return_train_score = True,\n",
        "                                  refit=True,\n",
        "                                  n_jobs=-1,  # Number of CPU cores to use (-1 means all available cores)\n",
        "                                  verbose=0, **cv_params[pipeline]\n",
        "                                 )\n",
        "  opts[pipeline].fit(X, y,callback=custom_callback)\n",
        "  models[pipeline]=opts[pipeline].best_estimator_\n",
        "  results[pipeline]=pd.DataFrame.from_dict(opts[pipeline].cv_results_)\n",
        "\n",
        "  print(f\"Best hyperparameters for {pipeline}:\", opts[pipeline].best_params_)\n",
        "  print(f\"Best cross-validation score for {pipeline}:\", opts[pipeline].best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MSBzY1xapMOj"
      },
      "outputs": [],
      "source": [
        "#@title View results { run: \"auto\" }\n",
        "\n",
        "features = \"xrd+geo\" #@param [\"xrd+geo\",\"xrd\",\"best_anova\",\"best_rf\"]\n",
        "\n",
        "method = \"t-sne\" #@param [\"pca\",\"t-sne\",\"umap\"]\n",
        "target = \"y__hi_toc\" #@param [\"y__hi_toc\", \"y__hi_if\", \"y__hi_toc&if\"]\n",
        "show_labels = 'false'\n",
        "\n",
        "feature_sets = {'xrd+geo': columns_geological + columns_xrd ,\n",
        "                'best_anova': best_features['ANOVA']}\n",
        "\n",
        "fig, axes = plt.subplots(1,len(pipelines))\n",
        "fig.set_size_inches(20,6)\n",
        "fig.suptitle('Model Prediction (' + train_features + ')', fontsize=15)\n",
        "\n",
        "# select features\n",
        "cols = feature_sets[features]\n",
        "df_plot=compute_reduced_dimension(df, cols, method=method)\n",
        "\n",
        "for idx, pipeline in enumerate(pipelines):\n",
        "\n",
        "  # Get best columns\n",
        "\n",
        "  # test scores\n",
        "  best_results=results[pipeline][results[pipeline]['rank_test_score']==1]\n",
        "  test_score_cols=['split'+ str(split) + '_test_score' for split in range( opts[pipeline].n_splits_)  ]\n",
        "  df_plot.loc[ df_plot[target].notnull(),'test_score_'+pipeline]=np.array(best_results[test_score_cols].mean()).astype('float64')\n",
        "\n",
        "  # train scores\n",
        "  best_results=results[pipeline][results[pipeline]['rank_train_score']==1]\n",
        "  train_score_cols=['split'+ str(split) + '_train_score' for split in range( opts[pipeline].n_splits_)  ]\n",
        "  df_plot.loc[ df_plot[target].notnull(),'train_score_'+pipeline]=np.array(best_results[test_score_cols].mean()).astype('float64')\n",
        "\n",
        "  # probability (TODO: average between all ???)\n",
        "  features=opts[pipeline].feature_names_in_\n",
        "  df_plot['prob_'+pipeline+'__'+target]=opts[pipeline].best_estimator_.predict_proba(df_plot[features]).astype('float64')[:,1]\n",
        "\n",
        "  # prediction (TODO: average mayority between all ???)\n",
        "  df_plot['pred_'+pipeline+'__'+target]=opts[pipeline].best_estimator_.predict(df_plot[features]).astype('int')\n",
        "\n",
        "  # Sweep over q values and plot resuts\n",
        "  axes[idx].set_title(pipeline)# + '\\nDimension reduction method: ' + method )\n",
        "\n",
        "  # Original samples\n",
        "  # plot_2d(axes[idx], df_plot, method + '_1'  , method + '_2' , target ,'section', 'sample', show_labels = 'false')\n",
        "\n",
        "  # Prediction\n",
        "\n",
        "  # Define the categories\n",
        "  import matplotlib.cm as cm\n",
        "  import itertools\n",
        "\n",
        "  categories = df_plot[target].unique()\n",
        "  palette = sns.color_palette(\"Set1\")\n",
        "  palette_iter = itertools.cycle(palette)\n",
        "  category_colors = {category: next(palette_iter) for category in categories}\n",
        "\n",
        "\n",
        "  # pred contour\n",
        "  sns.kdeplot(df_plot,  x= method+'_1'  , y=method+'_2', hue='pred_'+pipeline+'__'+target, legend=False, palette=category_colors, alpha=0.5, ax=axes[idx])\n",
        "\n",
        "  # target\n",
        "  sizes = {True: 100, False: 10,  None: 0}\n",
        "  sns.scatterplot(df_plot,  x= method+'_1'  , y=method+'_2', hue=target ,size=target, palette=category_colors, legend=True ,marker='o', ax=axes[idx], sizes=sizes)\n",
        "\n",
        "  # pred points\n",
        "  pred_colors=df_plot['pred_'+pipeline+'__'+target].map(category_colors)\n",
        "  try:\n",
        "    sns.scatterplot(df_plot,\n",
        "                    x= method+'_1',\n",
        "                    y=method+'_2',\n",
        "                    ec=pred_colors,\n",
        "                    fc=\"none\",\n",
        "                    legend=True,\n",
        "                    size='prob_'+pipeline+'__'+target, marker='o',\n",
        "                    linewidth=1,\n",
        "                    ax=axes[idx])\n",
        "  except  Exception as e:\n",
        "    print(\"An error occurred:\", e)\n",
        "\n",
        "  _x=method+'_1'\n",
        "  _y=method+'_2'\n",
        "\n",
        "  from adjustText import adjust_text as at\n",
        "  if show_labels == \"true\":\n",
        "    texts = []\n",
        "    labels = df_plot['sample'].astype(str)\n",
        "    for i, txt in enumerate(labels):\n",
        "        texts.append(axes[idx].text(df_plot.at[i,_x], df_plot.at[i,_y], txt, ha='center',alpha=1, fontsize=7))\n",
        "\n",
        "    # Adjust text positions to avoid overlap\n",
        "    at(texts, ax=axes[idx])\n",
        "    \n",
        "  fig.savefig(f'diagrams/prediction_{train_features}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MfctF1_ZQYX7"
      },
      "outputs": [],
      "source": [
        "#@title Accuracy { run: \"auto\" }\n",
        "\n",
        "fig, axes = plt.subplots(1,len(pipelines))\n",
        "fig.set_size_inches(20,6)\n",
        "fig.suptitle('Accuracy' + ' (' + train_features + ')', fontsize=15)\n",
        "\n",
        "accuracies = {}\n",
        "\n",
        "for idx, pipeline in enumerate(pipelines):\n",
        "\n",
        "  data=results[pipeline]\n",
        "\n",
        "  data['model_number'] = data.reset_index().index + 1\n",
        "\n",
        "  df_list=[]\n",
        "\n",
        "  source_df = pd.DataFrame(data)\n",
        "\n",
        "  for ds in ('train','test'):\n",
        "\n",
        "    melted_df = pd.melt(source_df, id_vars=['model_number','rank_test_score','params'],\n",
        "                        value_vars=[col for col in source_df.columns if 'split' in col and ds+'_score' in col],\n",
        "                        var_name='split_score', value_name= 'score')\n",
        "\n",
        "    melted_df['split'] = melted_df['split_score'].str.extract(r'split(\\d+)').astype(int)\n",
        "    melted_df['train_test'] = melted_df['split_score'].str.extract(r'split\\d+_(\\w+)_score')\n",
        "\n",
        "    melted_df.drop(columns=['split_score'], inplace=True)\n",
        "\n",
        "    df_list.append(melted_df)\n",
        "\n",
        "  #df_pivot=pd.concat(df_list, ignore_index=True)\n",
        "  df_pivot=pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "  accuracies[pipeline] = df_pivot\n",
        "\n",
        "  sns.pointplot(df_pivot,\n",
        "                x='rank_test_score',\n",
        "                y='score',\n",
        "                hue='train_test',\n",
        "                errorbar=('ci', 95),\n",
        "                ax=axes[idx])\n",
        "  axes[idx].set_ylim([0.3, 1.02])\n",
        "  axes[idx].set_title(pipeline)\n",
        "\n",
        "\n",
        "  fig.savefig(f'diagrams/accuracy_curves_{train_features}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create a list of DataFrames with a new column 'key' representing the dictionary key\n",
        "dfs_with_key = []\n",
        "\n",
        "for key, df_accuracy in accuracies.items():\n",
        "    df_accuracy['model'] = key\n",
        "    dfs_with_key.append(df_accuracy)\n",
        "    \n",
        "df_accuracies = pd.concat(dfs_with_key, ignore_index=True)\n",
        "\n",
        "df_best_accuracies = df_accuracies[df_accuracies['rank_test_score']==1]\n",
        "\n",
        "df_average_scores = df_best_accuracies.groupby(['model','train_test'])['score'].agg(['mean', 'std', 'count']).reset_index()\n",
        "\n",
        "# Calculate the 95% confidence interval\n",
        "df_average_scores['ci95'] = 1.96 * (df_average_scores['std'] / np.sqrt(df_average_scores['count']))\n",
        "\n",
        "df_average_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot using seaborn\n",
        "\n",
        "# Use Tab10 color palette\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "plt.suptitle('Train/Test Accuracy' + ' (' + train_features + ')', fontsize=15)\n",
        "\n",
        "ax = sns.barplot(x='model', y='score', hue='train_test', errorbar=('ci', 95), data=df_best_accuracies, capsize=0.2)\n",
        "#for i in ax.containers:\n",
        "#    ax.bar_label(i,fmt='%.3f')\n",
        "\n",
        "# Annotate bars with labels\n",
        "for p in ax.patches:\n",
        "    ax.annotate(format(p.get_height(), '.3f'), \n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()-0.2), \n",
        "                ha = 'center', va = 'center', \n",
        "                xytext = (0, 9), \n",
        "                textcoords = 'offset points')\n",
        "    \n",
        "# Add labels and title\n",
        "plt.xlabel('model')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(title='Train/Test', loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.ylim(0.3,1.02)\n",
        "\n",
        "\n",
        "plt.savefig(f'diagrams/accuracy_{train_features}.png')\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
